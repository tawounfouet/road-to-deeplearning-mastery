{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawounfouet/road-to-deeplearning-mastery/blob/main/Text_Generation_With_LSTM_Recurrent_Neural_Networks_in_Python_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh02sOhwis4Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn-m-YXWix5M"
      },
      "source": [
        "# Text Generation With LSTM Recurrent Neural Networks in Python with Keras\n",
        "\n",
        "\n",
        "Recurrent neural networks can also be used as generative models.\n",
        "\n",
        "Generative models like this are useful not only to study how well a model has learned a problem but also to learn more about the problem domain itself.\n",
        "\n",
        "\n",
        "[Source du projet](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNVHa5AEjBXN"
      },
      "source": [
        "## Problem Description: Project Gutenberg\n",
        "\n",
        "Many of the classical texts are no longer protected under copyright.\n",
        "\n",
        "This means you can download all the text for these books for free and use them in experiments, like creating generative models. Perhaps the best place to get access to free books that are no longer protected by copyright is Project Gutenberg.\n",
        "\n",
        "\n",
        "In this tutorial, you will use a favorite book from childhood as the dataset: .\n",
        "\n",
        "[Alice’s Adventures in Wonderland by Lewis Carroll](https://www.gutenberg.org/ebooks/11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNyP_GtsjkS6"
      },
      "source": [
        "## Develop a Small LSTM Recurrent Neural Network\n",
        "In this section, you will develop a **simple LSTM network** to learn sequences of characters from Alice in Wonderland. In the next section, you will use this model to generate new sequences of characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2syJgfnkM7P",
        "outputId": "d1adb28c-000f-4ca4-fadc-1d1575880897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-09 14:15:16--  https://www.gutenberg.org/cache/epub/11/pg11.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 174385 (170K) [text/plain]\n",
            "Saving to: ‘pg11.txt’\n",
            "\n",
            "pg11.txt            100%[===================>] 170.30K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-03-09 14:15:16 (2.39 MB/s) - ‘pg11.txt’ saved [174385/174385]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/cache/epub/11/pg11.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NCvFu6Akob_"
      },
      "outputs": [],
      "source": [
        "# rename the downloaded file\n",
        "#mv [options] source_file destination_file\n",
        "! mv pg11.txt wonderland.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jEfUAqMiu_2"
      },
      "outputs": [],
      "source": [
        "# importing the classes and functions we will use to train your model.\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGoKVpnqivC-",
        "outputId": "69c7c2a5-66bb-4861-e21e-cd31f471f4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "﻿the project gutenberg ebook of alice's adventures in wonderland\n",
            "    \n",
            "this ebook is for the use of anyone anywhere in the united states and\n",
            "most other parts of the world at no cost and with almost no \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "print(raw_text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsTgKdgRk2fH"
      },
      "outputs": [],
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55Cc2ri5lJue",
        "outputId": "ff93460c-1f96-4d3a-f503-6dc1005e6d52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('\\n', 0),\n",
              " (' ', 1),\n",
              " ('!', 2),\n",
              " ('#', 3),\n",
              " ('$', 4),\n",
              " ('%', 5),\n",
              " (\"'\", 6),\n",
              " ('(', 7),\n",
              " (')', 8),\n",
              " ('*', 9)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(char_to_int.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69ID6IoBlbfF",
        "outputId": "8e5bd259-75df-4fb6-a236-9e390073757b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Characters : 163947\n",
            "Total Vocab: , 65\n"
          ]
        }
      ],
      "source": [
        "#. now that the book has been loaded and the mapping prepared, we can summarize the dataset.\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(f\"Total Characters : {n_chars}\")\n",
        "print(f\"Total Vocab: , {n_vocab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1A-Xvi0mu0V"
      },
      "source": [
        "We can see the book has just around 160,000 characters, and when converted to lowercase, there are only 67 distinct characters in the vocabulary for the network to learn—much more than the 26 in the alphabet\n",
        "\n",
        "\n",
        "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
        "\n",
        "In this project, we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. we could just as easily split the data by sentences, padding the shorter sequences and truncating the longer ones.\n",
        "\n",
        "Each training pattern of the network comprises 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters, of course).\n",
        "\n",
        "For example, if the sequence length is 5 (for simplicity), then the first two training patterns would be as follows:\n",
        "\n",
        "- CHAPT -> E\n",
        "- HAPTE -> R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRyQcJcfmSVZ",
        "outputId": "53c2ea01-4329-49d1-dfc8-517fa5ab89db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Patterns: , 163847\n"
          ]
        }
      ],
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "  seq_in = raw_text[i:i + seq_length]\n",
        "  seq_out = raw_text[i + seq_length]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print(f\"Total Patterns: , {n_patterns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dkbIkTCno0z",
        "outputId": "a9c39fc1-cc8c-4cbd-fa5b-844d51bfc3bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[64, 49, 37, 34, 1, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 1, 34, 31, 44, 44, 40, 1, 44, 35, 1, 30, 41, 38, 32, 34, 6, 48, 1, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 1, 38, 43, 1, 52, 44, 43, 33, 34, 47, 41, 30, 43, 33, 0, 1, 1, 1, 1, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35, 1, 30], [49, 37, 34, 1, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 1, 34, 31, 44, 44, 40, 1, 44, 35, 1, 30, 41, 38, 32, 34, 6, 48, 1, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 1, 38, 43, 1, 52, 44, 43, 33, 34, 47, 41, 30, 43, 33, 0, 1, 1, 1, 1, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35, 1, 30, 43], [37, 34, 1, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 1, 34, 31, 44, 44, 40, 1, 44, 35, 1, 30, 41, 38, 32, 34, 6, 48, 1, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 1, 38, 43, 1, 52, 44, 43, 33, 34, 47, 41, 30, 43, 33, 0, 1, 1, 1, 1, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35, 1, 30, 43, 54]]\n"
          ]
        }
      ],
      "source": [
        "print(dataX[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFVx8I82nsDI"
      },
      "outputs": [],
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDktPCkdoiaw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IefeOqyQoozq"
      },
      "outputs": [],
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFY8TRI3orZH",
        "outputId": "92132ab5-1d93-44c3-9ee4-eb05a3e8df84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 256)               264192    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                16448     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 280640 (1.07 MB)\n",
            "Trainable params: 280640 (1.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "159k5ra1ouls"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T39R8-nRpBWD"
      },
      "outputs": [],
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBryut2WpD-t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8t7gEa2pJm4"
      },
      "source": [
        "We can now fit your model to the data. Here, you use a modest number of 20 epochs and a large batch size of 128 patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmQI7MKhpHEs",
        "outputId": "1b51eff9-4461-40df-f369-e286e2266506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1281/1281 [==============================] - ETA: 0s - loss: 3.0141\n",
            "Epoch 1: loss improved from inf to 3.01410, saving model to weights-improvement-01-3.0141.hdf5\n",
            "1281/1281 [==============================] - 823s 641ms/step - loss: 3.0141\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1281/1281 [==============================] - ETA: 0s - loss: 2.8474\n",
            "Epoch 2: loss improved from 3.01410 to 2.84742, saving model to weights-improvement-02-2.8474.hdf5\n",
            "1281/1281 [==============================] - 807s 630ms/step - loss: 2.8474\n",
            "Epoch 3/20\n",
            "1281/1281 [==============================] - ETA: 0s - loss: 2.7682\n",
            "Epoch 3: loss improved from 2.84742 to 2.76822, saving model to weights-improvement-03-2.7682.hdf5\n",
            "1281/1281 [==============================] - 808s 631ms/step - loss: 2.7682\n",
            "Epoch 4/20\n",
            "1281/1281 [==============================] - ETA: 0s - loss: 2.7049\n",
            "Epoch 4: loss improved from 2.76822 to 2.70493, saving model to weights-improvement-04-2.7049.hdf5\n",
            "1281/1281 [==============================] - 801s 625ms/step - loss: 2.7049\n",
            "Epoch 5/20\n",
            " 578/1281 [============>.................] - ETA: 7:19 - loss: 2.6656"
          ]
        }
      ],
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MllfQuM_pOrL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# load the network weights\n",
        "filename = \"weights-improvement-19-1.9435.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        " x = np.reshape(pattern, (1, len(pattern), 1))\n",
        " x = x / float(n_vocab)\n",
        " prediction = model.predict(x, verbose=0)\n",
        " index = np.argmax(prediction)\n",
        " result = int_to_char[index]\n",
        " seq_in = [int_to_char[value] for value in pattern]\n",
        " sys.stdout.write(result)\n",
        " pattern.append(index)\n",
        " pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHEOkmpIxmIICBHyLOACGA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}